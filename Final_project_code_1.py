# -*- coding: utf-8 -*-
"""Final project code_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wYTJcM9PNcpEJzM9II-gngH4jZY9i4hK
"""

# Part 1: Data Preprocessing & Feature Engineering

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

# Load the dataset
application_data = pd.read_csv('application_data.csv')

import pandas as pd

# Load your dataset
# application_data = pd.read_csv("application_data.csv")

# ---------------------------
# Basic Cleaning
# ---------------------------
print("Original Data Info:")
print(application_data.info())

# Handling missing values
numeric_columns = application_data.select_dtypes(include=['float64', 'int64']).columns
categorical_columns = application_data.select_dtypes(include=['object']).columns

# Fill missing numeric values with median
application_data[numeric_columns] = application_data[numeric_columns].fillna(application_data[numeric_columns].median())

# Fill missing categorical values with mode
for col in categorical_columns:
    application_data[col].fillna(application_data[col].mode()[0], inplace=True)

# Remove duplicates
application_data.drop_duplicates(inplace=True)

# ---------------------------
# Feature Engineering
# ---------------------------

# Convert 'DAYS_BIRTH' to 'AGE' in years (positive values)
application_data['AGE'] = (-application_data['DAYS_BIRTH']) / 365
application_data['AGE'] = application_data['AGE'].round().astype(int)

# Create clean age group categories with no overlap
bins = [18, 26, 36, 46, 56, 66, float('inf')]
labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '66+']
application_data['AGE_GROUP'] = pd.cut(application_data['AGE'], bins=bins, labels=labels, right=False)
application_data['AGE_GROUP'] = pd.Categorical(application_data['AGE_GROUP'], categories=labels, ordered=True)

print("\nAGE and AGE_GROUP sample:")
print(application_data[['AGE', 'AGE_GROUP']].head(10))

# Create a Credit to Income Ratio feature
application_data['CREDIT_INCOME_RATIO'] = application_data['AMT_CREDIT'] / (application_data['AMT_INCOME_TOTAL'] + 1)

# Create Employment to Age Ratio
application_data['EMPLOY_AGE_RATIO'] = (-application_data['DAYS_EMPLOYED']) / (application_data['DAYS_BIRTH'] + 1)

# ---------------------------
# Sum of Ages in Each AGE_GROUP
# ---------------------------
age_group_age_sum = application_data.groupby('AGE_GROUP')['AGE'].sum()
print("\nTotal AGE sum within each AGE_GROUP:")
print(age_group_age_sum)

# ---------------------------
# Final Output
# ---------------------------
print("\nData after Cleaning and Feature Engineering:")
print(application_data.head())

# Save cleaned and feature-engineered data for next parts
application_data.to_csv('processed_application_data.csv', index=False)

# Part 2: Handling Class Imbalance (SMOTE) and Train/Test Split

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# -----------------------------
# Load Processed Data
# -----------------------------
application_data = pd.read_csv('processed_application_data.csv')

# -----------------------------
# Prepare Features and Target
# -----------------------------
X = application_data.drop(['TARGET', 'SK_ID_CURR'], axis=1)  # Remove ID and target
y = application_data['TARGET']

# Encode categorical variables (one-hot encoding, drop first to avoid multicollinearity)
X = pd.get_dummies(X, drop_first=True)

# -----------------------------
# Print Original Shape and Target Distribution
# -----------------------------
print(f"Shape of feature set before SMOTE: {X.shape}")
print(f"Target distribution before SMOTE:\n{y.value_counts()}")

# -----------------------------
# Train/Test Split (Before SMOTE)
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

print(f"\nShape of training set before SMOTE: {X_train.shape}")
print(f"Shape of test set: {X_test.shape}")
print(f"Training target distribution before SMOTE:\n{y_train.value_counts()}")
print(f"Test target distribution:\n{y_test.value_counts()}")

# -----------------------------
# Apply SMOTE to Training Set
# -----------------------------
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print(f"\nShape after SMOTE:")
print(f"X_train_res: {X_train_res.shape}")
print(f"y_train_res distribution:\n{y_train_res.value_counts()}")

# -----------------------------
# Save All Sets for Modeling
# -----------------------------
X_train_res.to_csv('X_train_res.csv', index=False)
y_train_res.to_csv('y_train_res.csv', index=False)
X_test.to_csv('X_test.csv', index=False)
y_test.to_csv('y_test.csv', index=False)

# Part 3: Baseline Models - Logistic Regression & Decision Tree

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the processed train/test datasets
X_train = pd.read_csv('X_train_res.csv')
y_train = pd.read_csv('y_train_res.csv')
X_test = pd.read_csv('X_test.csv')
y_test = pd.read_csv('y_test.csv')

# Flatten y_train and y_test if necessary
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

# Logistic Regression Model
logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)

# Decision Tree Model
dtree = DecisionTreeClassifier(random_state=42)
dtree.fit(X_train, y_train)
y_pred_dtree = dtree.predict(X_test)

# Evaluation Function
def evaluate_model(y_true, y_pred, model_name):
    print(f"\nModel: {model_name}")
    print(confusion_matrix(y_true, y_pred))
    print(classification_report(y_true, y_pred))
    print(f"ROC-AUC Score: {roc_auc_score(y_true, y_pred):.4f}")

# Evaluate Logistic Regression
evaluate_model(y_test, y_pred_logreg, "Logistic Regression")

# Evaluate Decision Tree
evaluate_model(y_test, y_pred_dtree, "Decision Tree")

# Plot ROC Curve for both models
fpr_logreg, tpr_logreg, _ = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])
fpr_dtree, tpr_dtree, _ = roc_curve(y_test, dtree.predict_proba(X_test)[:,1])

plt.figure(figsize=(8,6))
plt.plot(fpr_logreg, tpr_logreg, label='Logistic Regression')
plt.plot(fpr_dtree, tpr_dtree, label='Decision Tree')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.grid()
plt.show()

# Part 4: Advanced Models - XGBoost & LightGBM (Clean Version)

import pandas as pd
import matplotlib.pyplot as plt
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

# Load processed train/test datasets
X_train = pd.read_csv('X_train_res.csv')
y_train = pd.read_csv('y_train_res.csv')
X_test = pd.read_csv('X_test.csv')
y_test = pd.read_csv('y_test.csv')

# Clean feature names to avoid LightGBM JSON errors
X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# Flatten target arrays
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

# Train XGBoost
xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

# Train LightGBM
lgb_model = lgb.LGBMClassifier(random_state=42)
lgb_model.fit(X_train, y_train)
y_pred_lgb = lgb_model.predict(X_test)

# Evaluation Function
def evaluate_model(y_true, y_pred, model_name):
    print(f"\n=== {model_name} ===")
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred))
    print(f"ROC-AUC Score: {roc_auc_score(y_true, y_pred):.4f}")

# Evaluate models
evaluate_model(y_test, y_pred_xgb, "XGBoost")
evaluate_model(y_test, y_pred_lgb, "LightGBM")

# Plot ROC curves
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_model.predict_proba(X_test)[:, 1])
fpr_lgb, tpr_lgb, _ = roc_curve(y_test, lgb_model.predict_proba(X_test)[:, 1])

plt.figure(figsize=(8, 6))
plt.plot(fpr_xgb, tpr_xgb, label='XGBoost')
plt.plot(fpr_lgb, tpr_lgb, label='LightGBM')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison: XGBoost vs LightGBM')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import shap
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance

# Load data
X_train = pd.read_csv('X_train_res.csv')
y_train = pd.read_csv('y_train_res.csv')
X_test = pd.read_csv('X_test.csv')
y_test = pd.read_csv('y_test.csv')

# Clean feature names to avoid LightGBM JSON parsing issues
X_train.columns = X_train.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)
X_test.columns = X_test.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# Flatten target arrays
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

# Train XGBoost
xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Train LightGBM
lgb_model = lgb.LGBMClassifier(random_state=42)
lgb_model.fit(X_train, y_train)

# -------- SHAP for XGBoost --------
print("\n✅ Generating SHAP feature importance plot for XGBoost...")
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Show top 20 SHAP feature importances
shap.summary_plot(shap_values, X_test, plot_type="bar", max_display=20)

# -------- Permutation Importance for LightGBM --------
print("\n✅ Calculating Permutation Importance for LightGBM (sampled full features)...")

# Sample a smaller subset of test data to reduce memory usage
sample_X = X_test.sample(n=3000, random_state=42)
sample_y = y_test[sample_X.index]

# No feature limitation now — use full feature set
perm_importance = permutation_importance(
    lgb_model,
    sample_X,  # FULL feature set
    sample_y,
    n_repeats=5,
    random_state=42,
    n_jobs=2
)

# Sort and plot top 10 important features
sorted_idx = perm_importance.importances_mean.argsort()[-10:]

plt.figure(figsize=(8, 6))
plt.barh(sample_X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance Score")
plt.title("Top 10 Important Features – LightGBM (Sampled)")
plt.grid(True)
plt.tight_layout()
plt.show()

print("\n✅ Feature Importance Analysis Completed Successfully.")

# Part 6: Interactive Visualizations with Plotly

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# Load processed data
application_data = pd.read_csv('processed_application_data.csv')

# Clean feature names (important for plotly readability)
application_data.columns = application_data.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# Remove unknown gender values (optional, cleaner plots)
application_data = application_data[application_data['CODE_GENDER'].isin(['M', 'F'])]

# 1. Interactive Histogram: Income Distribution by Default Status
fig_income = px.histogram(application_data,
                          x='AMT_INCOME_TOTAL',
                          color='TARGET',
                          nbins=50,
                          title='Income Distribution by Loan Default Status',
                          labels={'TARGET': 'Loan Default (0 = No, 1 = Yes)'},
                          color_discrete_sequence=['green', 'red'])

fig_income.update_layout(bargap=0.2)
fig_income.show()

# 2. Interactive Boxplot: Income by Gender and Loan Default
fig_box = px.box(application_data,
                 x='CODE_GENDER',
                 y='AMT_INCOME_TOTAL',
                 color='TARGET',
                 title='Income by Gender and Loan Default Status',
                 labels={'AMT_INCOME_TOTAL': 'Annual Income'},
                 color_discrete_sequence=['blue', 'orange'])

fig_box.show()

# 3. Interactive Scatter Plot: Income vs Credit Amount colored by Default
fig_scatter = px.scatter(application_data,
                         x='AMT_INCOME_TOTAL',
                         y='AMT_CREDIT',
                         color='TARGET',
                         title='Income vs Credit Amount by Loan Default',
                         hover_data=['AGE', 'AGE_GROUP'],
                         color_discrete_sequence=['green', 'red'])

fig_scatter.show()

# 4. Correlation Heatmap: Important Numeric Features
important_features = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AGE', 'CREDIT_INCOME_RATIO', 'EMPLOY_AGE_RATIO', 'TARGET']
correlation_matrix = application_data[important_features].corr()

fig_corr = go.Figure(data=go.Heatmap(
    z=correlation_matrix.values,
    x=correlation_matrix.columns,
    y=correlation_matrix.columns,
    colorscale='RdBu',
    zmin=-1, zmax=1))

fig_corr.update_layout(
    title='Correlation Heatmap of Key Features',
    xaxis_nticks=36)

fig_corr.show()

# Create credit bins based on quartiles
application_data['CREDIT_BIN'] = pd.qcut(application_data['AMT_CREDIT'], q=4, labels=['Low', 'Med-Low', 'Med-High', 'High'])

# Scatter plot
fig_scatter = px.strip(  # use strip to mimic scatter on categorical axis
    application_data,
    x='CREDIT_BIN',
    y='AMT_INCOME_TOTAL',
    color='TARGET',
    title='Scatter Plot of Income by Credit Amount Bin',
    labels={'CREDIT_BIN': 'Credit Amount Bin', 'AMT_INCOME_TOTAL': 'Annual Income'},
    hover_data=['AGE', 'AMT_CREDIT'],
    color_discrete_sequence=['green', 'red']
)

fig_scatter.show()

# Bin credit into categories first
application_data['CREDIT_BIN'] = pd.qcut(application_data['AMT_CREDIT'], q=4, labels=['Low', 'Med-Low', 'Med-High', 'High'])

fig_income_box = px.box(
    application_data,
    x='CREDIT_BIN',
    y='AMT_INCOME_TOTAL',
    title='Boxplot of Income by Credit Amount Bin',
    labels={'CREDIT_BIN': 'Credit Bin', 'AMT_INCOME_TOTAL': 'Annual Income'},
    color='CREDIT_BIN'
)
fig_income_box.show()

# Bin credit amount
application_data['CREDIT_BIN'] = pd.qcut(application_data['AMT_CREDIT'], q=4, labels=['Low', 'Med-Low', 'Med-High', 'High'])

# Now create a violin plot
fig_violin = px.violin(
    application_data,
    x='CREDIT_BIN',
    y='AMT_INCOME_TOTAL',
    box=True,
    points='all',
    title='Violin Plot of Income by Credit Amount Bin',
    color='TARGET',
    color_discrete_sequence=['green', 'red']
)
fig_violin.show()

import shap
shap.initjs()

explainer = shap.Explainer(xgb_model)
shap_values = explainer(X_test)

# Display force plot for index 0
shap.force_plot(explainer.expected_value, shap_values[0].values, X_test.iloc[0], matplotlib=True)

!pip install dash jupyter-dash plotly

import pandas as pd
import plotly.express as px
from dash import Dash, dcc, html, Input, Output

# Load data
application_data = pd.read_csv('processed_application_data.csv')
application_data.columns = application_data.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)

# Filter for Male/Female only
application_data = application_data[application_data['CODE_GENDER'].isin(['M', 'F'])]

# Build app
app = Dash(__name__)

# App Layout
app.layout = html.Div([
    html.H1("Loan Default Risk Dashboard", style={'textAlign': 'center'}),

    html.Div([
        html.Label("Select Gender:"),
        dcc.Dropdown(
            id='gender-dropdown',
            options=[
                {'label': 'All', 'value': 'All'},
                {'label': 'Male', 'value': 'M'},
                {'label': 'Female', 'value': 'F'}
            ],
            value='All'
        ),

        html.Label("Select Loan Default Status:"),
        dcc.Dropdown(
            id='default-dropdown',
            options=[
                {'label': 'All', 'value': 'All'},
                {'label': 'Non-Defaulter (0)', 'value': 0},
                {'label': 'Defaulter (1)', 'value': 1}
            ],
            value='All'
        ),
    ], style={'width': '48%', 'display': 'inline-block', 'padding': '10px'}),

    dcc.Graph(id='income-credit-scatter'),
    dcc.Graph(id='income-histogram')
])

# Callbacks
@app.callback(
    [Output('income-credit-scatter', 'figure'),
     Output('income-histogram', 'figure')],
    [Input('gender-dropdown', 'value'),
     Input('default-dropdown', 'value')]
)
def update_graphs(selected_gender, selected_default):
    filtered_data = application_data.copy()

    if selected_gender != 'All':
        filtered_data = filtered_data[filtered_data['CODE_GENDER'] == selected_gender]
    if selected_default != 'All':
        filtered_data = filtered_data[filtered_data['TARGET'] == int(selected_default)]

    scatter_fig = px.scatter(
        filtered_data,
        x='AMT_INCOME_TOTAL',
        y='AMT_CREDIT',
        color='TARGET',
        hover_data=['AGE', 'AGE_GROUP'],
        title='Income vs Credit Amount by Loan Default Status',
        color_discrete_sequence=['green', 'red']
    )

    hist_fig = px.histogram(
        filtered_data,
        x='AMT_INCOME_TOTAL',
        color='TARGET',
        nbins=50,
        title='Income Distribution by Loan Default Status',
        color_discrete_sequence=['green', 'red']
    )

    return scatter_fig, hist_fig

# Start server (New correct method!)
app.run(host="0.0.0.0", port=8050, debug=True)
